{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd3c8ed9",
   "metadata": {},
   "source": [
    "In this example, we're using an LSTM model with four layers, each with 50 units. We're also applying a dropout rate of 0.2 to each LSTM layer, which will randomly drop out 20% of the input units during training to prevent overfitting. Finally, we're adding a dense layer with a single output unit, which will be used to predict a single target value.\n",
    "\n",
    "To train the model, we're using the Adam optimizer and mean squared error loss function. We're also splitting the data into training and testing sets and using the validation_data parameter to evaluate the model's performance on the testing set after each epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b780a5b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dropout\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# define the model architecture\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "# generate a synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# reshape the input data to fit the expected input shape of the model\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units=32, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units=16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd7e41",
   "metadata": {},
   "source": [
    "In this example, we're using an LSTM model with four layers, each with a higher capacity compared to the previous example (128, 64, 32, 16 units, respectively). We're also applying a dropout rate of 0.5 to each LSTM layer, which will randomly drop out 50% of the input units during training to prevent overfitting. Finally, we're adding a dense layer with a single output unit, which will be used to predict a single target value.\n",
    "\n",
    "To train the model, we're using the Adam optimizer and mean squared error loss function. We're also splitting the data into training and testing sets and using the validation_data parameter to evaluate the model's performance on the testing set after each epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58564de1",
   "metadata": {},
   "source": [
    "### Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units=32, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units=16))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2c13e",
   "metadata": {},
   "source": [
    "In this example, we're using an LSTM model with four layers, each with a higher capacity compared to the previous example (128, 64, 32, 16 units, respectively). We're also applying a dropout rate of 0.5 to each LSTM layer, which will randomly drop out 50% of the input units during training to prevent overfitting. Finally, we're adding a dense layer with a single output unit, which will be used to predict a single target value.\n",
    "\n",
    "To train the model, we're using the Adam optimizer and mean squared error loss function. We're also splitting the data into training and testing sets and using the validation_data parameter to evaluate the model's performance on the testing set after each epoch of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb37a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
